{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# superclass of modules\n",
    "class Module:\n",
    "    \"\"\"\n",
    "    Module is a super class. It could be a single layer, or a multilayer perceptron.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.train = True\n",
    "        return\n",
    "    \n",
    "    def forward(self, _input):\n",
    "        \"\"\"\n",
    "        h = f(z); z is the input, and h is the output.\n",
    "        \n",
    "        Inputs:\n",
    "        _input: z\n",
    "        \n",
    "        Returns:\n",
    "        output h\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def backward(self, _input, _gradOutput):\n",
    "        \"\"\"\n",
    "        Compute:\n",
    "        gradient w.r.t. _input\n",
    "        gradient w.r.t. trainable parameters\n",
    "        \n",
    "        Inputs:\n",
    "        _input: z\n",
    "        _gradOutput: dL/dh\n",
    "        \n",
    "        Returns:\n",
    "        gradInput: dL/dz\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Return the value of trainable parameters and its corresponding gradient (Used for grandient descent)\n",
    "        \n",
    "        Returns:\n",
    "        params, gradParams\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def training(self):\n",
    "        \"\"\"\n",
    "        Turn the module into training mode.(Only useful for Dropout layer)\n",
    "        Ignore it if you are not using Dropout.\n",
    "        \"\"\"\n",
    "        self.train = True\n",
    "        \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Turn the module into evaluate mode.(Only useful for Dropout layer)\n",
    "        Ignore it if you are not using Dropout.\n",
    "        \"\"\"\n",
    "        self.train = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    \"\"\"\n",
    "    Sequential provides a way to plug layers together in a feed-forward manner.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        Module.__init__(self)\n",
    "        self.layers = [] # layers contain all the layers in order\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer) # Add another layer at the end\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.layers) # How many layers.\n",
    "    \n",
    "    def forward(self, _input):\n",
    "        \"\"\"\n",
    "        Feed forward through all the layers, and return the output of the last layer\n",
    "        \"\"\"\n",
    "        # self._inputs saves the input of each layer\n",
    "        # self._inputs[i] is the input of i-th layer\n",
    "        self._inputs = [_input]\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        for layer in self.layers:\n",
    "            _input = layer.forward(_input)\n",
    "            self._inputs.append(_input)\n",
    "        self._output = self._inputs[-1]\n",
    "\n",
    "        return self._output\n",
    "    \n",
    "    def backward(self, _input, _gradOutput):\n",
    "        \"\"\"\n",
    "        Backpropogate through all the layers using chain rule.\n",
    "        \"\"\"\n",
    "        # self._gradInputs[i] is the gradient of loss w.r.t. the input of i-th layer\n",
    "        self._gradInputs = [None] * (self.size() + 1)\n",
    "        self._gradInputs[self.size()] = _gradOutput\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        for i in reversed(range(self.size())):\n",
    "            _input = self._inputs[i]\n",
    "            _gradOutput = self.layers[i].backward(_input, _gradOutput)\n",
    "            self._gradInputs[i] = _gradOutput\n",
    "        self._gradInput = self._gradInputs[0]\n",
    "        \n",
    "        return self._gradInput\n",
    "    \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Return trainable parameters and its corresponding gradient in a nested list\n",
    "        \"\"\"\n",
    "        params = []\n",
    "        gradParams = []\n",
    "        for m in self.layers:\n",
    "            _p, _g = m.parameters()\n",
    "            if _p is not None:\n",
    "                params.append(_p)\n",
    "                gradParams.append(_g)\n",
    "        return params, gradParams\n",
    "\n",
    "    def training(self):\n",
    "        \"\"\"\n",
    "        Turn all the layers into training mode\n",
    "        \"\"\"\n",
    "        Module.training(self)\n",
    "        for m in self.layers:\n",
    "            m.training()\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Turn all the layers into evaluate mode\n",
    "        \"\"\"\n",
    "        Module.evaluate(self)\n",
    "        for m in self.layers:\n",
    "            m.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FullyConnected(Module):\n",
    "    \"\"\"\n",
    "    Fully connected layer\n",
    "    \"\"\"\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        Module.__init__(self)\n",
    "        # Initalization\n",
    "        stdv = 1./np.sqrt(inputSize)\n",
    "        \n",
    "        self.weight = np.random.uniform(-stdv, stdv, (inputSize, outputSize))\n",
    "        self.gradWeight = np.ndarray((inputSize, outputSize))\n",
    "        self.bias = np.random.uniform(-stdv, stdv, outputSize)\n",
    "        self.gradBias = np.ndarray(outputSize)\n",
    "        \n",
    "    def forward(self, _input):\n",
    "        \"\"\"\n",
    "        output = W * input + b\n",
    "        \n",
    "        _input:\n",
    "        N x inputSize matrix\n",
    "        \n",
    "        \"\"\"\n",
    "        self._output = np.dot(_input, self.weight) + self.bias # YOUR CODE HERE\n",
    "        return self._output\n",
    "    \n",
    "    def backward(self, _input, _gradOutput):\n",
    "        \"\"\"\n",
    "        _input:\n",
    "        N x inputSize matrix\n",
    "        _gradOutputSize:\n",
    "        N x outputSize matrix\n",
    "        \"\"\"\n",
    "        self.gradWeight = np.dot(_input.T, _gradOutput) # YOUR CODE HERE\n",
    "        self.gradBias = np.sum(_gradOutput, axis=0) # YOUR CODE HERE\n",
    "        \n",
    "        self._gradInput = np.dot(_gradOutput, self.weight.T) # YOUR CODE HERE\n",
    "        return self._gradInput\n",
    "        \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Return weight and bias and their g\n",
    "        \"\"\"\n",
    "        return [self.weight, self.bias], [self.gradWeight, self.gradBias]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    \"\"\"\n",
    "    ReLU activation, not trainable.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        Module.__init__(self)\n",
    "        return\n",
    "    \n",
    "    def forward(self, _input):\n",
    "        \"\"\"\n",
    "        output = max(0, input)\n",
    "        \n",
    "        _input:\n",
    "        N x d matrix\n",
    "        \"\"\"\n",
    "        self._output = np.maximum(0, _input) #YOUR CODE HERE\n",
    "        return self._output\n",
    "    \n",
    "    def backward(self, _input, _gradOutput):\n",
    "        \"\"\"\n",
    "        gradInput = gradOutput * mask\n",
    "        mask = _input > 0\n",
    "        \n",
    "        _input:\n",
    "        N x d matrix\n",
    "        \n",
    "        _gradOutput:\n",
    "        N x d matrix\n",
    "        \"\"\"\n",
    "        self._gradInput = _gradOutput * (_input > 0).astype(float) # YOUR CODE HERE\n",
    "        return self._gradInput\n",
    "        \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        No trainable parametersm, return None\n",
    "        \"\"\"\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optional\n",
    "class Logistic(Module):\n",
    "    \"\"\"\n",
    "    Logistic activation, not trainable.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        Module.__init__(self)\n",
    "        return\n",
    "    \n",
    "    def forward(self, _input):\n",
    "        self._output = 0 #YOUR CODE HERE\n",
    "        return self._output\n",
    "    \n",
    "    def backward(self, _input, _gradOutput):\n",
    "        self._gradInput = 0 #YOUR CODE HERE\n",
    "        return self._gradInput\n",
    "        \n",
    "    def parameters(self):\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional\n",
    "class Tanh(Module):\n",
    "    \"\"\"\n",
    "    Tanh activation, not trainable.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        Module.__init__(self)\n",
    "        return\n",
    "    \n",
    "    def forward(self, _input):\n",
    "        self._output = 0 #YOUR CODE HERE\n",
    "        return self._output\n",
    "    \n",
    "    def backward(self, _input, _gradOutput):\n",
    "        self._gradInput = 0 #YOUR CODE HERE\n",
    "        return self._gradInput\n",
    "        \n",
    "    def parameters(self):\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optional\n",
    "class Dropout(Module):\n",
    "    \"\"\"\n",
    "    A dropout layer\n",
    "    \"\"\"\n",
    "    def __init__(self, p = 0.5):\n",
    "        Module.__init__(self)\n",
    "        self.p = p #self.p is the drop rate, if self.p is 0, then it's a identity layer\n",
    "        \n",
    "    def forward(self, _input):\n",
    "        self._output = _input\n",
    "        # YOUR CODE HERE\n",
    "        # Need to take care of training mode and evaluation mode\n",
    "        return self._output\n",
    "    \n",
    "    def backward(self, _input, _gradOutput):\n",
    "        self._gradInput = _gradOutput\n",
    "        #YOUR CODE HERE\n",
    "        return self._gradInput\n",
    "    \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        No trainable parameters.\n",
    "        \"\"\"\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SoftMaxLoss(object):\n",
    "    def __init__(self):\n",
    "        return\n",
    "        \n",
    "    def forward(self, _input, _label):\n",
    "        \"\"\"\n",
    "        Softmax and cross entropy loss layer. Should return a scalar, since it's a\n",
    "        loss. (It's almost identical to what in hw2)\n",
    "\n",
    "        _input: N x C\n",
    "        _labels: N x C, one-hot\n",
    "\n",
    "        Returns: loss (scalar)\n",
    "        \"\"\"\n",
    "        #YOUR CODE HERE\n",
    "\n",
    "        exp_input = np.exp(_input - np.max(_input, axis=1, keepdims=True))\n",
    "        softmax = exp_input / np.sum(exp_input, axis=1, keepdims=True)\n",
    "        epsilon = 1e-9\n",
    "        self._output = -np.sum(_label * np.log(softmax + epsilon)) / len(_input)\n",
    "\n",
    "        return self._output\n",
    "    \n",
    "    def backward(self, _input, _label):\n",
    "        \n",
    "        exp_input = np.exp(_input - np.max(_input, axis=1, keepdims=True))\n",
    "        softmax = exp_input / np.sum(exp_input, axis=1, keepdims=True)\n",
    "        \n",
    "        self._gradInput = (softmax - _label) / len(_input) #YOUR CODE HERE\n",
    "        return self._gradInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.05680276190978e-09\n"
     ]
    }
   ],
   "source": [
    "# Test softmaxloss, the relative error should be small enough\n",
    "def test_sm():\n",
    "    crit = SoftMaxLoss()\n",
    "    gt = np.zeros((3, 10))\n",
    "    gt[np.arange(3), np.array([1,2,3])] = 1\n",
    "    x = np.random.random((3,10))\n",
    "    def test_f(x):\n",
    "        return crit.forward(x, gt)\n",
    "\n",
    "    crit.forward(x, gt)\n",
    "\n",
    "    gradInput = crit.backward(x, gt)\n",
    "    gradInput_num = numeric_gradient(test_f, x, 1, 1e-6)\n",
    "    #print(gradInput)\n",
    "    #print(gradInput_num)\n",
    "    print(relative_error(gradInput, gradInput_num, 1e-8))\n",
    "    \n",
    "test_sm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7523407956079448e-08\n",
      "5.962448038856628e-10\n",
      "1.4172931374988576e-08\n"
     ]
    }
   ],
   "source": [
    "# Test modules, all the relative errors should be small enough\n",
    "def test_module(model):\n",
    "\n",
    "    model.evaluate()\n",
    "\n",
    "    crit = TestCriterion()\n",
    "    gt = np.random.random((3,10))\n",
    "    x = np.random.random((3,10))\n",
    "    def test_f(x):\n",
    "        return crit.forward(model.forward(x), gt)\n",
    "\n",
    "    gradInput = model.backward(x, crit.backward(model.forward(x), gt))\n",
    "    gradInput_num = numeric_gradient(test_f, x, 1, 1e-6)\n",
    "    print(relative_error(gradInput, gradInput_num, 1e-8))\n",
    "\n",
    "# Test fully connected\n",
    "model = FullyConnected(10, 10)\n",
    "test_module(model)\n",
    "\n",
    "# Test ReLU\n",
    "model = ReLU()\n",
    "test_module(model)\n",
    "\n",
    "# Test Dropout\n",
    "# model = Dropout()\n",
    "# test_module(model)\n",
    "# You can only test dropout in evaluation mode.\n",
    "\n",
    "# Test Sequential\n",
    "model = Sequential()\n",
    "model.add(FullyConnected(10, 10))\n",
    "model.add(ReLU())\n",
    "#model.add(Dropout())\n",
    "test_module(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2095338882601591\n",
      "0.019452571838974732\n",
      "0.01747750382665477\n",
      "0.010995051318409246\n",
      "0.01455373056462134\n",
      "0.007505352700255541\n",
      "0.010238367867662784\n",
      "0.012505628683159381\n",
      "0.014465886106835423\n",
      "0.011445656341263368\n",
      "0.012812177436855915\n"
     ]
    }
   ],
   "source": [
    "# Test gradient descent, the loss should be lower and lower\n",
    "trainX = np.random.random((10,5))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(FullyConnected(5, 3))\n",
    "model.add(ReLU())\n",
    "#model.add(Dropout())\n",
    "model.add(FullyConnected(3, 1))\n",
    "\n",
    "crit = TestCriterion()\n",
    "\n",
    "it = 0\n",
    "state = None\n",
    "while True:\n",
    "    output = model.forward(trainX)\n",
    "    loss = crit.forward(output, None)\n",
    "    if it % 100 == 0:\n",
    "        print(loss)\n",
    "    doutput = crit.backward(output, None)\n",
    "    model.backward(trainX, doutput)\n",
    "    params, gradParams = model.parameters()\n",
    "    sgdmom(params, gradParams, 0.01, 0.8)\n",
    "    if it > 1000:\n",
    "        break\n",
    "    it += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start to work on real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load large trainset.\n",
      "(7000, 576)\n",
      "(7000, 10)\n",
      "Load valset.\n",
      "(2000, 576)\n",
      "(2000, 10)\n"
     ]
    }
   ],
   "source": [
    "import MNIST_utils\n",
    "data_fn = \"CLEAN_MNIST_SUBSETS.h5\"\n",
    "\n",
    "# We only consider large set this time\n",
    "print(\"Load large trainset.\")\n",
    "Xlarge,Ylarge = MNIST_utils.load_data(data_fn, \"large_train\")\n",
    "print(Xlarge.shape)\n",
    "print(Ylarge.shape)\n",
    "\n",
    "print(\"Load valset.\")\n",
    "Xval,Yval = MNIST_utils.load_data(data_fn, \"val\")\n",
    "print(Xval.shape)\n",
    "print(Yval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(X, model):\n",
    "    \"\"\"\n",
    "    Evaluate the soft predictions of the model.\n",
    "    Input:\n",
    "    X : N x d array (no unit terms)\n",
    "    model : a multi-layer perceptron\n",
    "    Output:\n",
    "    yhat : N x C array\n",
    "        yhat[n][:] contains the score over C classes for X[n][:]\n",
    "    \"\"\"\n",
    "    return model.forward(X)\n",
    "\n",
    "def error_rate(X, Y, model):\n",
    "    \"\"\"\n",
    "    Compute error rate (between 0 and 1) for the model\n",
    "    \"\"\"\n",
    "    model.evaluate()\n",
    "    res = 1 - (model.forward(X).argmax(-1) == Y.argmax(-1)).mean()\n",
    "    model.training()\n",
    "    return res\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "def runTrainVal(X,Y,model,Xval,Yval,trainopt):\n",
    "    \"\"\"\n",
    "    Run the train + evaluation on a given train/val partition\n",
    "    trainopt: various (hyper)parameters of the training procedure\n",
    "    During training, choose the model with the lowest validation error. (early stopping)\n",
    "    \"\"\"\n",
    "    \n",
    "    eta = trainopt['eta']\n",
    "    \n",
    "    N = X.shape[0] # number of data points in X\n",
    "    \n",
    "    # Save the model with lowest validation error\n",
    "    minValError = np.inf\n",
    "    saved_model = None\n",
    "    \n",
    "    shuffled_idx = np.random.permutation(N)\n",
    "    start_idx = 0\n",
    "    for iteration in range(trainopt['maxiter']):\n",
    "        if iteration % int(trainopt['eta_frac'] * trainopt['maxiter']) == 0:\n",
    "            eta *= trainopt['etadrop']\n",
    "        # form the next mini-batch\n",
    "        stop_idx = min(start_idx + trainopt['batch_size'], N)\n",
    "        batch_idx = range(N)[int(start_idx):int(stop_idx)]\n",
    "        bX = X[shuffled_idx[batch_idx],:]\n",
    "        bY = Y[shuffled_idx[batch_idx],:]\n",
    "\n",
    "        score = model.forward(bX)\n",
    "        loss = crit.forward(score, bY)\n",
    "        # print(loss)\n",
    "        dscore = crit.backward(score, bY)\n",
    "        model.backward(bX, dscore)\n",
    "        \n",
    "        # Update the data using \n",
    "        params, gradParams = model.parameters()\n",
    "        sgdmom(params, gradParams, eta, weight_decay = trainopt['lambda'])    \n",
    "        start_idx = stop_idx % N\n",
    "        \n",
    "        if (iteration % trainopt['display_iter']) == 0:\n",
    "            #compute train and val error; multiply by 100 for readability (make it percentage points)\n",
    "            trainError = 100 * error_rate(X, Y, model)\n",
    "            valError = 100 * error_rate(Xval, Yval, model)\n",
    "            print('{:8} batch loss: {:.3f} train error: {:.3f} val error: {:.3f}'.format(iteration, loss, trainError, valError))\n",
    "            \n",
    "            if valError < minValError:\n",
    "                saved_model = deepcopy(model)\n",
    "                minValError = valError\n",
    "        \n",
    "    return saved_model, minValError, trainError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model(input_size, hidden_size, output_size, activation_func = 'ReLU', dropout = 0):\n",
    "    \"\"\"\n",
    "    Build the model:\n",
    "    input_size: the dimension of input data\n",
    "    hidden_size: the dimension of hidden vector\n",
    "    output_size: the output size of final layer.\n",
    "    activation_func: ReLU, Logistic, Tanh, etc. (Need to be implemented by yourself)\n",
    "    dropout: the dropout rate: if dropout == 0, this is equivalent to no dropout\n",
    "    \"\"\"\n",
    "    model = Sequential() #YOUR CODE HERE\n",
    "\n",
    "    if hidden_size != 0:\n",
    "        model.add(FullyConnected(input_size, hidden_size))\n",
    "        if activation_func == 'ReLU':\n",
    "            model.add(ReLU())\n",
    "        if dropout != 0:\n",
    "            model.add(Dropout(dropout))\n",
    "    model.add(FullyConnected(hidden_size if hidden_size != 0 else input_size, output_size))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0 batch loss: 2.296 train error: 89.500 val error: 90.100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     500 batch loss: 0.431 train error: 11.229 val error: 10.300\n",
      "    1000 batch loss: 0.342 train error: 8.843 val error: 8.750\n",
      "    1500 batch loss: 0.302 train error: 7.429 val error: 7.950\n",
      "    2000 batch loss: 0.340 train error: 7.029 val error: 7.950\n",
      "    2500 batch loss: 0.349 train error: 6.557 val error: 7.700\n",
      "    3000 batch loss: 0.251 train error: 6.057 val error: 7.650\n",
      "    3500 batch loss: 0.155 train error: 5.686 val error: 7.200\n",
      "    4000 batch loss: 0.150 train error: 5.343 val error: 7.400\n",
      "    4500 batch loss: 0.187 train error: 5.029 val error: 7.400\n",
      "    5000 batch loss: 0.226 train error: 4.743 val error: 7.750\n",
      "    5500 batch loss: 0.236 train error: 4.500 val error: 7.500\n",
      "    6000 batch loss: 0.179 train error: 4.229 val error: 7.250\n",
      "    6500 batch loss: 0.176 train error: 4.329 val error: 7.350\n",
      "    7000 batch loss: 0.112 train error: 4.014 val error: 7.100\n",
      "    7500 batch loss: 0.098 train error: 4.029 val error: 7.450\n",
      "    8000 batch loss: 0.136 train error: 3.743 val error: 7.300\n",
      "    8500 batch loss: 0.198 train error: 3.700 val error: 7.600\n",
      "    9000 batch loss: 0.186 train error: 3.457 val error: 7.700\n",
      "    9500 batch loss: 0.133 train error: 3.314 val error: 7.700\n",
      "   10000 batch loss: 0.144 train error: 3.229 val error: 7.250\n",
      "   10500 batch loss: 0.092 train error: 3.129 val error: 7.200\n",
      "   11000 batch loss: 0.081 train error: 3.057 val error: 7.450\n",
      "   11500 batch loss: 0.119 train error: 2.814 val error: 7.150\n",
      "   12000 batch loss: 0.180 train error: 2.914 val error: 7.250\n",
      "   12500 batch loss: 0.160 train error: 2.829 val error: 7.350\n",
      "   13000 batch loss: 0.108 train error: 2.729 val error: 7.650\n",
      "   13500 batch loss: 0.130 train error: 2.714 val error: 7.500\n",
      "   14000 batch loss: 0.083 train error: 2.614 val error: 7.350\n",
      "   14500 batch loss: 0.074 train error: 2.643 val error: 7.600\n",
      "   15000 batch loss: 0.108 train error: 2.500 val error: 7.250\n",
      "   15500 batch loss: 0.171 train error: 2.514 val error: 7.300\n",
      "   16000 batch loss: 0.145 train error: 2.500 val error: 7.400\n",
      "   16500 batch loss: 0.096 train error: 2.457 val error: 7.450\n",
      "   17000 batch loss: 0.121 train error: 2.429 val error: 7.600\n",
      "   17500 batch loss: 0.078 train error: 2.443 val error: 7.400\n",
      "   18000 batch loss: 0.069 train error: 2.414 val error: 7.600\n",
      "   18500 batch loss: 0.104 train error: 2.371 val error: 7.450\n",
      "   19000 batch loss: 0.167 train error: 2.329 val error: 7.300\n",
      "   19500 batch loss: 0.136 train error: 2.357 val error: 7.450\n",
      "train set model: -> lambda= 0.0000, train error: 2.36, val error: 7.10\n",
      "       0 batch loss: 2.329 train error: 85.714 val error: 86.650\n",
      "     500 batch loss: 0.368 train error: 11.129 val error: 9.650\n",
      "    1000 batch loss: 0.434 train error: 8.771 val error: 8.400\n",
      "    1500 batch loss: 0.378 train error: 7.571 val error: 7.900\n",
      "    2000 batch loss: 0.277 train error: 7.086 val error: 8.000\n",
      "    2500 batch loss: 0.173 train error: 6.329 val error: 7.700\n",
      "    3000 batch loss: 0.256 train error: 6.100 val error: 7.800\n",
      "    3500 batch loss: 0.145 train error: 5.629 val error: 8.000\n",
      "    4000 batch loss: 0.188 train error: 5.529 val error: 7.800\n",
      "    4500 batch loss: 0.291 train error: 5.214 val error: 7.850\n",
      "    5000 batch loss: 0.193 train error: 4.957 val error: 7.650\n",
      "    5500 batch loss: 0.185 train error: 4.686 val error: 7.800\n",
      "    6000 batch loss: 0.123 train error: 4.571 val error: 7.800\n",
      "    6500 batch loss: 0.203 train error: 4.400 val error: 8.100\n",
      "    7000 batch loss: 0.110 train error: 4.329 val error: 8.300\n",
      "    7500 batch loss: 0.148 train error: 4.157 val error: 8.050\n",
      "    8000 batch loss: 0.239 train error: 4.100 val error: 7.850\n",
      "    8500 batch loss: 0.141 train error: 3.814 val error: 7.800\n",
      "    9000 batch loss: 0.147 train error: 3.743 val error: 8.250\n",
      "    9500 batch loss: 0.104 train error: 3.629 val error: 8.000\n",
      "   10000 batch loss: 0.171 train error: 3.500 val error: 8.300\n",
      "   10500 batch loss: 0.094 train error: 3.357 val error: 8.150\n",
      "   11000 batch loss: 0.126 train error: 3.271 val error: 8.250\n",
      "   11500 batch loss: 0.209 train error: 3.286 val error: 8.250\n",
      "   12000 batch loss: 0.114 train error: 3.257 val error: 8.400\n",
      "   12500 batch loss: 0.126 train error: 3.143 val error: 8.500\n",
      "   13000 batch loss: 0.092 train error: 3.143 val error: 8.350\n",
      "   13500 batch loss: 0.155 train error: 3.114 val error: 8.400\n",
      "   14000 batch loss: 0.088 train error: 3.143 val error: 8.350\n",
      "   14500 batch loss: 0.115 train error: 3.000 val error: 8.400\n",
      "   15000 batch loss: 0.192 train error: 2.986 val error: 8.350\n",
      "   15500 batch loss: 0.100 train error: 2.986 val error: 8.400\n",
      "   16000 batch loss: 0.114 train error: 2.900 val error: 8.450\n",
      "   16500 batch loss: 0.084 train error: 2.900 val error: 8.400\n",
      "   17000 batch loss: 0.145 train error: 2.871 val error: 8.550\n",
      "   17500 batch loss: 0.083 train error: 2.843 val error: 8.450\n",
      "   18000 batch loss: 0.109 train error: 2.757 val error: 8.550\n",
      "   18500 batch loss: 0.183 train error: 2.871 val error: 8.450\n",
      "   19000 batch loss: 0.095 train error: 2.786 val error: 8.500\n",
      "   19500 batch loss: 0.107 train error: 2.771 val error: 8.600\n",
      "train set model: -> lambda= 0.0010, train error: 2.77, val error: 7.65\n",
      "       0 batch loss: 2.273 train error: 85.400 val error: 84.200\n",
      "     500 batch loss: 0.316 train error: 11.257 val error: 10.150\n",
      "    1000 batch loss: 0.227 train error: 8.514 val error: 8.300\n",
      "    1500 batch loss: 0.238 train error: 7.500 val error: 7.850\n",
      "    2000 batch loss: 0.267 train error: 6.986 val error: 7.850\n",
      "    2500 batch loss: 0.128 train error: 6.043 val error: 7.350\n",
      "    3000 batch loss: 0.335 train error: 6.457 val error: 8.150\n",
      "    3500 batch loss: 0.166 train error: 5.386 val error: 7.200\n",
      "    4000 batch loss: 0.105 train error: 5.300 val error: 7.400\n",
      "    4500 batch loss: 0.168 train error: 4.686 val error: 7.450\n",
      "    5000 batch loss: 0.147 train error: 4.586 val error: 7.000\n",
      "    5500 batch loss: 0.162 train error: 4.300 val error: 7.150\n",
      "    6000 batch loss: 0.084 train error: 4.100 val error: 6.800\n",
      "    6500 batch loss: 0.249 train error: 4.143 val error: 7.300\n",
      "    7000 batch loss: 0.130 train error: 3.743 val error: 6.900\n",
      "    7500 batch loss: 0.084 train error: 3.700 val error: 7.100\n",
      "    8000 batch loss: 0.140 train error: 3.700 val error: 7.200\n",
      "    8500 batch loss: 0.119 train error: 3.814 val error: 7.150\n",
      "    9000 batch loss: 0.116 train error: 3.471 val error: 7.200\n",
      "    9500 batch loss: 0.069 train error: 3.200 val error: 6.950\n",
      "   10000 batch loss: 0.207 train error: 3.400 val error: 7.450\n",
      "   10500 batch loss: 0.116 train error: 3.071 val error: 7.200\n",
      "   11000 batch loss: 0.073 train error: 3.043 val error: 7.250\n",
      "   11500 batch loss: 0.122 train error: 2.971 val error: 7.150\n",
      "   12000 batch loss: 0.106 train error: 3.114 val error: 7.150\n",
      "   12500 batch loss: 0.098 train error: 2.943 val error: 7.350\n",
      "   13000 batch loss: 0.061 train error: 2.843 val error: 7.200\n",
      "   13500 batch loss: 0.187 train error: 2.843 val error: 7.550\n",
      "   14000 batch loss: 0.108 train error: 2.600 val error: 7.300\n",
      "   14500 batch loss: 0.067 train error: 2.714 val error: 7.250\n",
      "   15000 batch loss: 0.113 train error: 2.700 val error: 7.100\n",
      "   15500 batch loss: 0.100 train error: 2.643 val error: 7.300\n",
      "   16000 batch loss: 0.088 train error: 2.629 val error: 7.250\n",
      "   16500 batch loss: 0.056 train error: 2.600 val error: 7.300\n",
      "   17000 batch loss: 0.174 train error: 2.514 val error: 7.300\n",
      "   17500 batch loss: 0.103 train error: 2.471 val error: 7.350\n",
      "   18000 batch loss: 0.064 train error: 2.529 val error: 7.200\n",
      "   18500 batch loss: 0.107 train error: 2.514 val error: 7.250\n",
      "   19000 batch loss: 0.095 train error: 2.500 val error: 7.350\n",
      "   19500 batch loss: 0.083 train error: 2.500 val error: 7.450\n",
      "train set model: -> lambda= 0.0100, train error: 2.50, val error: 6.80\n",
      "       0 batch loss: 2.275 train error: 89.786 val error: 91.350\n",
      "     500 batch loss: 0.459 train error: 11.057 val error: 9.800\n",
      "    1000 batch loss: 0.348 train error: 8.671 val error: 8.450\n",
      "    1500 batch loss: 0.406 train error: 7.686 val error: 7.750\n",
      "    2000 batch loss: 0.297 train error: 6.743 val error: 7.750\n",
      "    2500 batch loss: 0.239 train error: 6.371 val error: 7.350\n",
      "    3000 batch loss: 0.161 train error: 5.843 val error: 7.100\n",
      "    3500 batch loss: 0.285 train error: 5.443 val error: 7.250\n",
      "    4000 batch loss: 0.194 train error: 5.043 val error: 7.300\n",
      "    4500 batch loss: 0.219 train error: 5.057 val error: 7.400\n",
      "    5000 batch loss: 0.284 train error: 4.400 val error: 6.950\n",
      "    5500 batch loss: 0.171 train error: 4.157 val error: 7.000\n",
      "    6000 batch loss: 0.154 train error: 4.043 val error: 7.100\n",
      "    6500 batch loss: 0.133 train error: 3.857 val error: 7.050\n",
      "    7000 batch loss: 0.212 train error: 3.914 val error: 7.250\n",
      "    7500 batch loss: 0.119 train error: 3.686 val error: 7.000\n",
      "    8000 batch loss: 0.171 train error: 3.786 val error: 7.550\n",
      "    8500 batch loss: 0.230 train error: 3.457 val error: 7.100\n",
      "    9000 batch loss: 0.133 train error: 3.329 val error: 6.850\n",
      "    9500 batch loss: 0.123 train error: 3.186 val error: 6.950\n",
      "   10000 batch loss: 0.122 train error: 3.143 val error: 7.050\n",
      "   10500 batch loss: 0.181 train error: 3.171 val error: 7.100\n",
      "   11000 batch loss: 0.090 train error: 3.114 val error: 7.100\n",
      "   11500 batch loss: 0.146 train error: 3.071 val error: 7.200\n",
      "   12000 batch loss: 0.194 train error: 2.929 val error: 6.950\n",
      "   12500 batch loss: 0.112 train error: 2.829 val error: 7.050\n",
      "   13000 batch loss: 0.107 train error: 2.800 val error: 6.950\n",
      "   13500 batch loss: 0.112 train error: 2.786 val error: 6.900\n",
      "   14000 batch loss: 0.166 train error: 2.814 val error: 6.950\n",
      "   14500 batch loss: 0.081 train error: 2.757 val error: 7.200\n",
      "   15000 batch loss: 0.133 train error: 2.757 val error: 7.200\n",
      "   15500 batch loss: 0.173 train error: 2.643 val error: 7.150\n",
      "   16000 batch loss: 0.100 train error: 2.671 val error: 7.000\n",
      "   16500 batch loss: 0.099 train error: 2.629 val error: 7.050\n",
      "   17000 batch loss: 0.105 train error: 2.629 val error: 7.050\n",
      "   17500 batch loss: 0.156 train error: 2.586 val error: 7.150\n",
      "   18000 batch loss: 0.076 train error: 2.557 val error: 7.150\n",
      "   18500 batch loss: 0.124 train error: 2.543 val error: 6.950\n",
      "   19000 batch loss: 0.165 train error: 2.514 val error: 7.100\n",
      "   19500 batch loss: 0.096 train error: 2.514 val error: 7.000\n",
      "train set model: -> lambda= 0.1000, train error: 2.51, val error: 6.85\n",
      "lambda= 0.0000, hidden size:    10, val error: 7.10\n",
      "lambda= 0.0010, hidden size:    10, val error: 7.65\n",
      "lambda= 0.0100, hidden size:    10, val error: 6.80\n",
      "lambda= 0.1000, hidden size:    10, val error: 6.85\n",
      "Best train model val err: 6.799999999999995\n",
      "Best train model lambda: 0.01\n"
     ]
    }
   ],
   "source": [
    "# -- training options\n",
    "trainopt = {\n",
    "    'eta': .1,   # initial learning rate\n",
    "    'maxiter': 20000,   # max number of iterations (updates) of SGD\n",
    "    'display_iter': 500,  # display batch loss every display_iter updates\n",
    "    'batch_size': 100,  \n",
    "    'etadrop': .5, # when dropping eta, multiply it by this number (e.g., .5 means halve it)\n",
    "    'eta_frac': .25  #\n",
    "}\n",
    "\n",
    "NFEATURES = Xlarge.shape[1]\n",
    "\n",
    "# we will maintain a record of models trained for different values of lambda\n",
    "# these will be indexed directly by lambda value itself\n",
    "trained_models = dict()\n",
    "\n",
    "# set the (initial?) set of lambda values to explore\n",
    "lambdas = np.array([0, 0.001, 0.01, 0.1])\n",
    "hidden_sizes = np.array([10])\n",
    "    \n",
    "for lambda_ in lambdas:\n",
    "    for hidden_size_ in hidden_sizes:\n",
    "        trainopt['lambda'] = lambda_\n",
    "        model = build_model(NFEATURES, hidden_size_, 10, dropout = 0)\n",
    "        crit = SoftMaxLoss()\n",
    "        # -- model trained on large train set\n",
    "        trained_model,valErr,trainErr = runTrainVal(Xlarge, Ylarge, model, Xval, Yval, trainopt)\n",
    "        trained_models[(lambda_, hidden_size_)] = {'model': trained_model, \"val_err\": valErr, \"train_err\": trainErr }\n",
    "        print('train set model: -> lambda= %.4f, train error: %.2f, val error: %.2f' % (lambda_, trainErr, valErr))\n",
    "    \n",
    "best_trained_lambda = 0.\n",
    "best_trained_model = None\n",
    "best_trained_val_err = 100.\n",
    "for (lambda_, hidden_size_), results in trained_models.items():\n",
    "    print('lambda= %.4f, hidden size: %5d, val error: %.2f' %(lambda_, hidden_size_, results['val_err']))\n",
    "    if results['val_err'] < best_trained_val_err:\n",
    "        best_trained_val_err = results['val_err']\n",
    "        best_trained_model = results['model']\n",
    "        best_trained_lambda = lambda_\n",
    "\n",
    "print(\"Best train model val err:\", best_trained_val_err)\n",
    "print(\"Best train model lambda:\", best_trained_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: submission-mnist.csv\n"
     ]
    }
   ],
   "source": [
    "#Generate a Kaggle submission file using `model`\n",
    "kaggleX = MNIST_utils.load_data(data_fn, 'kaggle')\n",
    "kaggleYhat = predict(kaggleX, best_trained_model).argmax(-1)\n",
    "save_submission('submission-mnist.csv', kaggleYhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used ReLU as the activation function as this is the more common activation function used for the MNIST problem. I have included the validation errors as well as the best train model validation error and lambda value for the three different update algorithms. On Kaggle, the model that used sgdmom performed the best.\n",
    "\n",
    "sgd:\n",
    "\n",
    "lambda= 0.0000, hidden size:    10, val error: 6.90\n",
    "lambda= 0.0010, hidden size:    10, val error: 6.60\n",
    "lambda= 0.0100, hidden size:    10, val error: 7.25\n",
    "lambda= 0.1000, hidden size:    10, val error: 7.60\n",
    "Best train model val err: 6.599999999999994\n",
    "Best train model lambda: 0.001\n",
    "\n",
    "sgdm:\n",
    "\n",
    "lambda= 0.0000, hidden size:    10, val error: 7.75\n",
    "lambda= 0.0010, hidden size:    10, val error: 7.25\n",
    "lambda= 0.0100, hidden size:    10, val error: 7.40\n",
    "lambda= 0.1000, hidden size:    10, val error: 7.00\n",
    "Best train model val err: 6.999999999999995\n",
    "Best train model lambda: 0.1\n",
    "\n",
    "sgdmom:\n",
    "\n",
    "lambda= 0.0000, hidden size:    10, val error: 7.10\n",
    "lambda= 0.0010, hidden size:    10, val error: 7.65\n",
    "lambda= 0.0100, hidden size:    10, val error: 6.80\n",
    "lambda= 0.1000, hidden size:    10, val error: 6.85\n",
    "Best train model val err: 6.799999999999995\n",
    "Best train model lambda: 0.01"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
